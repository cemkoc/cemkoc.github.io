<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
  <meta name=viewport content="width=device-width, initial-scale=1">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
  </style>
  <title>Cem Koc</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  </head>
  <body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
      <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"> -->
      <!-- <tr>
        <td width="67%" valign="middle">
        <p align="center">
          <name>Cem Koc</name>
        </p>
        <p>I am a fourth year undergraduate student studying electrical engineering and computer science at UC Berkeley. <a href="https://research.google.com/">Google Research</a>, where I work on computer vision and computational photography. At Google I've worked on <a href="http://googleresearch.blogspot.com/2014/04/lens-blur-in-new-google-camera-app.html">Lens Blur</a>, <a href="http://googleresearch.blogspot.com/2014/10/hdr-low-light-and-high-dynamic-range.html">HDR+</a>, <a href="https://www.google.com/get/cardboard/jump/">Jump</a>, and <a href="https://www.youtube.com/watch?v=JSnB06um5r4">Glass</a>.
        </p>
        <p>
          I did my PhD at <a href="http://www.eecs.berkeley.edu/">UC Berkeley</a>, where I was advised by <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a> and funded by the <a href="http://www.nsfgrfp.org/">NSF GRFP</a>. I've spent time at <a href="https://en.wikipedia.org/wiki/Google_X">Google[x]</a>, <a href="http://groups.csail.mit.edu/vision/welcome/">MIT CSAIL</a>, <a href="http://www.captricity.com/">Captricity</a>, <a href="http://ti.arc.nasa.gov/tech/asr/intelligent-robotics/">NASA Ames</a>, <a href="http://www.google.com/">Google NYC</a>, the <a href="http://mrl.nyu.edu/">NYU MRL</a>, <a href="http://www.nibr.com/">Novartis</a>, and <a href="http://www.astrometry.net/">Astrometry.net</a>. I did my bachelors at the <a href="http://cs.toronto.edu">University of Toronto</a>.
        </p>
        <p align=center>
        </p>
        </td>
        <td width="33%">
        <img src="JonBarron_circle.jpg">
        </td>
      </tr> -->
      <!-- </table> -->
      <!-- Undergraduate Research Section -->
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Undergradute Research</heading>
          <p>
          My undergraduate research involved working with mobile milli-robots for autonomous navigation, mapping and learning of complex terrains as well as control using sensory feedback.
          I worked under Prof. Ronald Fearing in the Biomimetic Milli-systems Lab part of Berkeley AI Research (BAIR)
          </p>
        </td>
      </tr>
      </table>
      <table width="100%" align="center" border="0" cellpadding="20">
        <tr>
            <td width="25%"><img src="/assets/images/velociroach.gif" alt="prl" width="160" height="160"></td>
            <td width="75%" valign="top">
            <p>
              <a href="https://arxiv.org/abs/1904.09101" target="_blank">
              <papertitle>Body Lift and Drag for a Legged Millirobot in Compliant Beam Environment</papertitle>
              </a>
              <br>
              <strong>Cem Koc*</strong>,
              Can Koc*,
              Brian Su*,
              <em>In ICRA 2019</em><br>
              <a href="/files/icra19_poster_final.pdf" target="_blank">poster</a>,
              <a href="https://arxiv.org/abs/1904.09101" target="_blank">arxiv</a>,
              <a href="https://www.youtube.com/watch?v=Bz6By7JliA8&feature=youtu.be" target="_blank">video</a>
    
              <p><br>
                Much current study of legged locomotion has rightly focused on foot traction forces, including on granular media. 
                Future legged millirobots will need to go through terrain, such as brush or other vegetation, where the body contact 
                forces significantly affect locomotion. In this work, a (previously developed) low-cost 6-axis force/torque sensing 
                shell is used to measure the interaction forces between a hexapedal millirobot and a set of compliant beams, which 
                act as a surrogate for a densely cluttered environment. Experiments with a VelociRoACH robotic platform are used to 
                measure lift and drag forces on the tactile shell, where negative lift forces can increase traction, even while drag 
                forces increase. The drag energy and specific resistance required to pass through dense terrains can be measured. 
                Furthermore, some contact between the robot and the compliant beams can lower specific resistance of locomotion. 
                For small, light-weight legged robots in the beam environment, the body motion depends on both leg-ground and 
                body-beam forces. A shell-shape which reduces drag but increases negative lift, such as the half-ellipsoid used, 
                is suggested to be advantageous for robot locomotion in this type of environment.
              </p>
            </p>
            </td>
        </tr>
      </table>
      <table width="100%" align="center" border="0" cellpadding="20">
        <tr onmouseout="bars_stop()" onmouseover="bars_start()" >
          <td width="25%">

            <div class="one">
                <div class="two" id = 'bars_image'><img src='/assets/images/bars_robot_small.jpg'></div>
                <img src='/assets/images/bars_robot_small.jpg'>
            </div>
            <script type="text/javascript">
            function bars_start() {
              document.getElementById('bars_image').style.opacity = "1";
            }
            function bars_stop() {
              document.getElementById('bars_image').style.opacity = "0";
            }
            bars_stop()
            </script>

          </td>
          <td valign="top" width="75%">
          <p><a href="/files/bars_poster.pdf" target="_blank">
            <papertitle>Terrain Classification with Force-Torque Sensor Equipped Millirobot</papertitle></a><br>
            <strong>Cem Koc*,</strong>
            Can Koc*,
            Brian Su*,
            Ron S. Fearing,
            <em>Presented at Bay Area Robotics Symposium (BARS)</em>, 2016<br>
            <a href ="/files/bars_poster.pdf" target="_blank">poster</a>

            <p></p>
            <p>
              Using a low-cost 6DOF force-torque sensing shell we can correctly classify dense terrain
              utilizing machine learning and ensemble learning from time series sensor data.
            </p>
            <p></p>
          </a></p>
          </td>
        </tr>
      </table>
      <!-- Projects Section -->
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
        <heading>Projects</heading>
        <p>
          Here you can find some of the projects I've worked on. I am very interested in developing projects which are at the intersection of Machine Learning, 
          Distributed/Parallel Computing and Big Data Analytics. Specifically interested in designing distributed frameworks for machine learning, ETL pipelines 
          and high performance computing. I also love graphs especially when they don't have negative cycles.
        </p>
        </td>
      </tr>
      </table>
      <table width="100%" align="center" border="0" cellpadding="20">
        <tr>
          <!-- <td width="25%"><img src="/assets/images/cs267_img1.png" alt="prl" width="280" height="160"></td> -->
          <td width="25%"><img src="/assets/images/cs267_img2.png" alt="prl" width="160" height="160"></td>
          <td width="75%" valign="top">
            <p>
              <a href="/files/CS_267_Final_Project_Report_Cem_Koc_Eric_Liu.pdf" target="_blank">
                <papertitle>Parallelizing Chamfer Distance Computation for Point Cloud Similarity</papertitle>
              </a>
              <br>
              <strong>Cem Koc</strong>,
              Eric Liu,
              Spring 2021<br>
              <a href="/files/chamfer_distance_slide.pdf" target="_blank">slide</a>,
              <a href="/files/CS_267_Final_Project_Report_Cem_Koc_Eric_Liu.pdf" target="_blank">report</a>
            <p><br>
              Given the increasing prevalence of point cloud data from LiDAR sensors, efficient processing and manipulation of
              massive amounts of this data have become credibly important. Chamfer distance metric is an important similarity metric
              when doing comparisons between two point clouds scans often of the same scene. We developed a way to make this
              computation much faster by re-structing the problem as a parallel computing problem. We utilized both kdtree and octrees
              to avoid the quadratic nature of the problem and implemented four different Chamfer distance algorithms including
              two approximation based algorithms using OpenMP and CUDA to explore the potential for increased computational efficiency.
              We compared it with the naive implementation and saw that our implementations on both OpenMP and CUDA significantly improved
              the naive implementations boding well for consumer or industry applications.
            </p>
            </p>
          </td>
        </tr>
      </table>
      <table width="100%" align="center" border="0" cellpadding="20">
      <tr>
        <td width="25%"><img src="/assets/images/atsushi_dnn.gif" alt="prl" width="160" height="160"></td>
        <td width="75%" valign="top">
        <p>
          <a href="/files/CS280_ProjectPoster.pdf" target="_blank">
          <papertitle>Detecting Faces in Animes Using Supervised Domain Adaptation</papertitle>
          </a>
          <br>
          <strong>Cem Koc*</strong>,
          Can Koc*,
          Brian Su*,
          <em>CS280: Special Topics in Computer Vision (graduate level)</em>, Spring 2016<br>
          <a href="/files/CS280_ProjectPoster.pdf" target="_blank">poster</a>,
          <a href="/files/Detecting_Anime_Faces.pdf" target="_blank">report</a>

          <p><br>
            We created our own anime images dataset and labeled them whether they contain face or not.
            Using GoogleNet, AlexNet, VGGFace models pre-trained on ImageNet 2014 we did supervised domain adaptation on
            these models by fine-tuning them on our anime faces dataset from various layers. Achieved ~97% accuracy.
          </p>
        </p>
        </td>
      </tr>
      </table>
      <table width="100%" align="center" border="0" cellpadding="20">
      <tr>
        <td width="25%"><img src="/assets/images/vizdoom_shooting.gif" alt="prl" width="160" height="160"></td>
        <!-- <td width="25%">
          <div style="max-width: 200px;" id="_giphy_vizdoomshooting"></div>
          <script>
           var _giphy_play_on_hover = true; var _giphy = _giphy || []; _giphy.push({id: "vizdoomshooting",w: 480, h: 290, clickthrough_url: "/assets/images/vizdoom_shooting.gif"});var g = document.createElement("script"); g.type = "text/javascript"; g.async = true;g.src = ("https:" == document.location.protocol ? "https://" : "http://") + "giphy.com/static/js/widgets/embed.js";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(g, s);
          </script>
        </td> -->
        <td width="75%" valign="top">
        <p>
          <a href="/files/cs294-129_poster.pdf" target="_blank">
          <papertitle>Prioritized Experience Replay Kills on ViZDoom</papertitle>
          </a>
          <br>
          <strong>Cem Koc*</strong>,
          Can Koc*,
          Brian Su*,
          <em>CS294-129: Special Topics in Designing, Visualizing and Understanding Deep Neural Networks (graduate level)</em>, Fall 2016<br>
          <a href="/files/cs294-129_poster.pdf" target="_blank">poster</a>


          <p><br>
            Implemented Prioritized Experience Replay from 2016 <a href="https://arxiv.org/abs/1511.05952">paper</a>
            from Schaul et al. to play on ViZDoom. Our results have shown that Prioritized Experience replay
            significantly reduces training time on both dense and sparse reward games. Benchamarked it
            with DQN model from the original DeepMind <a href="https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf">paper</a>.
            Compared to vanilla DQN our model trains much faster.
          </p>
        </p>
        </td>
      </tr>
      </table>
      <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
        <heading>Teaching</heading>
        </td>
      </tr>
      </table> -->
      <!-- Add Google Analytics  -->
          <!-- Google Analytics Tracking code -->
  <script>
     (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
     (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
     m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
     })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

     ga('create', 'UA-96359340-1', 'auto');
     ga('send', 'pageview');

   </script>
    </td>
    </tr>
  </table>
  </body>
</html>
